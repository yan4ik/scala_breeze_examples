{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DZ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                         \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.scalanlp::breeze:1.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mbreeze.linalg.{DenseMatrix, DenseVector, *, sum, max}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mbreeze.stats.{mean, stddev}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mbreeze.numerics.pow\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import breeze.linalg.{DenseMatrix, DenseVector, *, sum, max}\n",
    "import breeze.stats.{mean, stddev}\n",
    "import breeze.numerics.pow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mDATA_FILE\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"./Dataset.csv\"\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val DATA_FILE = \"./Dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's look [here](https://archive.ics.uci.edu/ml/datasets/Facebook+Comment+Volume+Dataset) what features are we dealing with:\n",
    " * we see that **output** is the target column.\n",
    " * we see that most of the columns are **numerical** but some are **categorical**.\n",
    " \n",
    "Note that the columns at the link do not directly correspond to the kaggle dataset, so some I made some guessing.\n",
    "\n",
    "For simplicity we will treat categorical featuers as numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcategory_col\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m3\u001b[39m\n",
       "\u001b[36mtarget_col\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m-1\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val category_col = 3\n",
    "val target_col = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mnumber_of_features\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m28\u001b[39m\n",
       "\u001b[36mnumber_of_samples\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m40949\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val number_of_features = io.Source.fromFile(DATA_FILE).getLines.next().split(\",\").length\n",
    "val number_of_samples = io.Source.fromFile(DATA_FILE).getLines.drop(1).length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">data</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">DenseMatrix</span></span>[<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Double</span></span>] = 634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    65.0  ... (28 total)\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    10.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    14.0  ...\n",
       "634995.0  0.0  463.0  1.0  7.0   0.0   3.0   7.0   -3.0   62.0  ...\n",
       "634995.0  0.0  463.0  1.0  1.0   0.0   0.0   1.0   0.0    58.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    60.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    68.0  ...\n",
       "634995.0  0.0  463.0  1.0  1.0   0.0   1.0   1.0   -1.0   32.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    35.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    48.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    52.0  ...\n",
       "634995.0  0.0  463.0  1.0  1.0   0.0   0.0   1.0   0.0    69.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    3.0   ...\n",
       "634995.0  0.0  463.0  1.0  1.0   1.0   0.0   1.0   1.0    37.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    23.0  ...\n",
       "634995.0  0.0  463.0  1.0  3.0   0.0   3.0   3.0   -3.0   40.0  ...\n",
       "634995.0  0.0  463.0  1.0  3.0   0.0   3.0   2.0   -3.0   54.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    29.0  ...\n",
       "634995.0  0.0  463.0  1.0  1.0   0.0   1.0   1.0   -1.0   36.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    11.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    62.0  ...\n",
       "634995.0  0.0  463.0  1.0  77.0  46.0  31.0  74.0  15.0   31.0  ...\n",
       "634995.0  0.0  463.0  1.0  6.0   0.0   1.0   6.0   -1.0   54.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    69.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    71.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    61.0  ...\n",
       "634995.0  0.0  463.0  1.0  4.0   4.0   0.0   4.0   4.0    13.0  ...\n",
       "634995.0  0.0  463.0  1.0  29.0  3.0   11.0  25.0  -8.0   49.0  ...\n",
       "634995.0  0.0  463.0  1.0  3.0   1.0   2.0   3.0   -1.0   35.0  ...\n",
       "634995.0  0.0  463.0  1.0  2.0   2.0   0.0   2.0   2.0    11.0  ...\n",
       "634995.0  0.0  463.0  1.0  2.0   0.0   2.0   2.0   -2.0   37.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    0.0   ...\n",
       "634995.0  0.0  463.0  1.0  2.0   2.0   0.0   2.0   2.0    11.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    43.0  ...\n",
       "634995.0  0.0  463.0  1.0  2.0   2.0   0.0   2.0   2.0    18.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    64.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    51.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    67.0  ...\n",
       "634995.0  0.0  463.0  1.0  8.0   8.0   0.0   8.0   8.0    12.0  ...\n",
       "...\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">count</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Int</span></span> = <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">40949</span></span>\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">nan_mask</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">DenseMatrix</span></span>[<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Double</span></span>] = 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... (28 total)\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "...</code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mDenseMatrix\u001b[39m[\u001b[32mDouble\u001b[39m] = 634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    65.0  ... (28 total)\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    10.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    14.0  ...\n",
       "634995.0  0.0  463.0  1.0  7.0   0.0   3.0   7.0   -3.0   62.0  ...\n",
       "634995.0  0.0  463.0  1.0  1.0   0.0   0.0   1.0   0.0    58.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    60.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    68.0  ...\n",
       "634995.0  0.0  463.0  1.0  1.0   0.0   1.0   1.0   -1.0   32.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    35.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    48.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    52.0  ...\n",
       "634995.0  0.0  463.0  1.0  1.0   0.0   0.0   1.0   0.0    69.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    3.0   ...\n",
       "634995.0  0.0  463.0  1.0  1.0   1.0   0.0   1.0   1.0    37.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    23.0  ...\n",
       "634995.0  0.0  463.0  1.0  3.0   0.0   3.0   3.0   -3.0   40.0  ...\n",
       "634995.0  0.0  463.0  1.0  3.0   0.0   3.0   2.0   -3.0   54.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    29.0  ...\n",
       "634995.0  0.0  463.0  1.0  1.0   0.0   1.0   1.0   -1.0   36.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    11.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    62.0  ...\n",
       "634995.0  0.0  463.0  1.0  77.0  46.0  31.0  74.0  15.0   31.0  ...\n",
       "634995.0  0.0  463.0  1.0  6.0   0.0   1.0   6.0   -1.0   54.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    69.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    71.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    61.0  ...\n",
       "634995.0  0.0  463.0  1.0  4.0   4.0   0.0   4.0   4.0    13.0  ...\n",
       "634995.0  0.0  463.0  1.0  29.0  3.0   11.0  25.0  -8.0   49.0  ...\n",
       "634995.0  0.0  463.0  1.0  3.0   1.0   2.0   3.0   -1.0   35.0  ...\n",
       "634995.0  0.0  463.0  1.0  2.0   2.0   0.0   2.0   2.0    11.0  ...\n",
       "634995.0  0.0  463.0  1.0  2.0   0.0   2.0   2.0   -2.0   37.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    0.0   ...\n",
       "634995.0  0.0  463.0  1.0  2.0   2.0   0.0   2.0   2.0    11.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    43.0  ...\n",
       "634995.0  0.0  463.0  1.0  2.0   2.0   0.0   2.0   2.0    18.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    64.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    51.0  ...\n",
       "634995.0  0.0  463.0  1.0  0.0   0.0   0.0   0.0   0.0    67.0  ...\n",
       "634995.0  0.0  463.0  1.0  8.0   8.0   0.0   8.0   8.0    12.0  ...\n",
       "...\n",
       "\u001b[36mcount\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m40949\u001b[39m\n",
       "\u001b[36mnan_mask\u001b[39m: \u001b[32mDenseMatrix\u001b[39m[\u001b[32mDouble\u001b[39m] = 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... (28 total)\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...\n",
       "..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val data = DenseMatrix.zeros[Double](number_of_samples, number_of_features)\n",
    "\n",
    "var count = 0\n",
    "for (line <- io.Source.fromFile(DATA_FILE).getLines.drop(1)) {\n",
    "    data(count, ::) := DenseVector(line.split(\",\").map(_.trim).map(x => if (x == \"\") 0 else x.toDouble)).t\n",
    "    count += 1\n",
    "}\n",
    "\n",
    "val nan_mask = DenseMatrix.zeros[Double](number_of_samples, number_of_features)\n",
    "\n",
    "count = 0\n",
    "for (line <- io.Source.fromFile(DATA_FILE).getLines.drop(1)) {\n",
    "    nan_mask(count, ::) := DenseVector(line.split(\",\").map(_.trim).map(x => if (x == \"\") 1.0 else 0.0)).t\n",
    "    count += 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's see how many columns contain missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres6\u001b[39m: \u001b[32mbreeze\u001b[39m.\u001b[32mlinalg\u001b[39m.\u001b[32mTranspose\u001b[39m[\u001b[32mDenseVector\u001b[39m[\u001b[32mDouble\u001b[39m]] = \u001b[33mTranspose\u001b[39m(\n",
       "  DenseVector(0.0, 0.0, 51.0, 57.0, 60.0, 0.0, 48.0, 0.0, 0.0, 0.0, 0.0, 2449.0, 0.0, 0.0, 1927.0, 0.0, 0.0, 3045.0, 0.0, 0.0, 0.0, 1970.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_mask(::, *).map( sum(_) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some columns have many mssing values, but not too much - so we can keep all features.\n",
    "\n",
    "The missing values values will be substituted by the mean value in the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's look at the variance of our features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mvariances\u001b[39m: \u001b[32mbreeze\u001b[39m.\u001b[32mlinalg\u001b[39m.\u001b[32mTranspose\u001b[39m[\u001b[32mDenseVector\u001b[39m[\u001b[32mDouble\u001b[39m]] = \u001b[33mTranspose\u001b[39m(\n",
       "  DenseVector(6785751.74784735, 20593.184863050174, 110933.80077100082, 19.95744994414867, 136.97888741476675, 77.12426279172584, 71.07861335688017, 128.1799197535962, 94.20297361088377, 20.916863962096603, 376.2643865983319, 945.0224135766251, 1.9198288062251618, 0.32769050657584886, 0.32539679105629726, 0.35682785499601277, 0.3642652465210098, 0.3312862964463732, 0.3532682733054544, 0.34377385303781793, 0.3486843746618664, 0.2779866662410545, 0.3445202882295503, 0.35569757158449067, 0.3579032471191856, 0.3509786877221571, 0.3519917166397198, 35.49454978260473)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val variances = data(::, *).map( stddev(_) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all columns, except some binary, have a pretty noticable variance - since we will be using linear regression **we really need to normalize data** (including the target column)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmean_with_nans\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mget_stats\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mnormalize\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_with_nans(v: DenseMatrix[Double], mask: DenseMatrix[Double]): DenseVector[Double] = {\n",
    "    val uncorrected_means = v(::, *).map( mean(_) ).t\n",
    "    val nans_per_column = mask(::, *).map( sum(_) ).t\n",
    "    \n",
    "    val v_rows = DenseVector.fill(v.cols){v.rows.toDouble}\n",
    "    \n",
    "    uncorrected_means *:* (v_rows /:/ (v_rows - nans_per_column))\n",
    "}\n",
    "\n",
    "def get_stats(v: DenseMatrix[Double], nan_mask: DenseMatrix[Double]): (DenseVector[Double], DenseVector[Double]) = {\n",
    "    val means = mean_with_nans(v, nan_mask)\n",
    "    val replaced_nan = v + nan_mask(*, ::) *:* means\n",
    "    val variances = replaced_nan(::, *).map( stddev(_) ).t\n",
    " \n",
    "    (means, variances)\n",
    "}\n",
    "    \n",
    "def normalize(v: DenseMatrix[Double],\n",
    "              nan_mask: DenseMatrix[Double],\n",
    "              means: DenseVector[Double], \n",
    "              variances: DenseVector[Double]): DenseMatrix[Double] = {\n",
    "    val replaced_nan = v + nan_mask(*, ::) *:* means\n",
    "    val zero_mean = (replaced_nan(*, ::) - means)\n",
    "    zero_mean(*, ::) /:/ variances\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtrain\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(data: DenseMatrix[Double], \n",
    "          target: DenseVector[Double], \n",
    "          lr: Double, \n",
    "          n_iter: Int, \n",
    "          l_2: Double): DenseVector[Double] = {    \n",
    "    (1 to n_iter).foldLeft(DenseVector.rand(data.cols))((model, _) => {\n",
    "        val grad = data.t * (data * model - target)\n",
    "        model - (lr / data.rows) * grad - l_2 * 2.0 * model\n",
    "    })\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be our **core quality metric**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mscore\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mr2_score\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def score(prediction: DenseVector[Double], target: DenseVector[Double]): Double =\n",
    "    mean(pow(target - prediction, 2))\n",
    "\n",
    "def r2_score(model: DenseVector[Double], data: DenseMatrix[Double], target: DenseVector[Double]): Double =\n",
    "    1 - score(data * model, target) / score(DenseVector.fill(target.size){mean(target)}, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtest_idx\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcross_val\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_idx(n: Int, k: Int): IndexedSeq[Range.Inclusive] = {\n",
    "    val test_size = n / k\n",
    "    for (i <- 0 to k-1) yield (test_size * i to test_size * (i + 1))\n",
    "}\n",
    "\n",
    "def cross_val(data: DenseMatrix[Double], nan_mask: DenseMatrix[Double], k: Int): IndexedSeq[(DenseMatrix[Double], \n",
    "                                                              DenseMatrix[Double],\n",
    "                                                              DenseMatrix[Double],\n",
    "                                                              DenseMatrix[Double])] = {\n",
    "    for (val_range <- test_idx(data.rows, k)) yield {\n",
    "        val train_range = (0 to val_range.start-1).toVector ++ (val_range.end+1 to data.rows-1).toVector\n",
    "        (data(train_range, ::).toDenseMatrix, \n",
    "         nan_mask(train_range, ::).toDenseMatrix,\n",
    "         data(val_range, ::),\n",
    "         nan_mask(val_range, ::)\n",
    "        )\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mprepare_data\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_data(data: DenseMatrix[Double]): (DenseMatrix[Double], DenseVector[Double]) = {\n",
    "    val x = data(::, 0 to data.cols-2)\n",
    "    val y = data(::, data.cols-1)\n",
    "    \n",
    "    val ones = DenseMatrix.fill(x.rows, 1){1.0}\n",
    "    val x_bias = DenseMatrix.horzcat(x, ones)\n",
    "    \n",
    "    (x_bias, y)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aug 17, 2020 2:20:40 PM com.github.fommil.netlib.BLAS <clinit>\n",
      "WARNING: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "Aug 17, 2020 2:20:40 PM com.github.fommil.netlib.BLAS <clinit>\n",
      "WARNING: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3990045102918133\n",
      "0.30891786446271596\n",
      "0.2705334113238851\n",
      "0.28536960451660387\n",
      "0.32618876674395725\n",
      "0.2526833784089181\n",
      "0.3011412068192183\n",
      "0.05977286292723738\n",
      "0.35098073129392604\n",
      "0.2951355234652727\n"
     ]
    }
   ],
   "source": [
    "for ((train_data, train_nan_mask, valid_data, valid_nan_mask) <- cross_val(data, nan_mask, 10)) {\n",
    "    val (mean, variances) = get_stats(train_data, train_nan_mask)\n",
    "    val train_norm = normalize(train_data, train_nan_mask, mean, variances)\n",
    "    val valid_norm = normalize(valid_data, valid_nan_mask, mean, variances)\n",
    "\n",
    "    \n",
    "    val (train_x, train_y) = prepare_data(train_norm)\n",
    "    val model = train(train_x, \n",
    "                      train_y, \n",
    "                      0.55,\n",
    "                      75, \n",
    "                      0.01)\n",
    "    \n",
    "    val (val_x, val_y) = prepare_data(valid_norm)\n",
    "    println(r2_score(model, val_x, val_y))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks close to what is achieved [here](https://www.kaggle.com/vinayvk1808/facebook-comment-dataset-using-random-forest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13",
   "language": "scala",
   "name": "scala213"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
